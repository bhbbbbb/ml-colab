{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework2: Convolution Neural Network for classification","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Homework2: Convolution Neural Network for classification\n","\n","In this homework, we are going to learn:\n","1. How to preprocess and load data in pytorch\n","2. How to build a CNN model for training classification\n","3. Training/Validation Process and plot the result.\n","4. Testing Inference.\n","\n","<p align=\"center\">\n","<img src=\"https://miro.medium.com/max/895/1*RjZe7cfnhdRhhLimLvapow.png\" width=\"800\">\n","</p>"],"metadata":{"id":"tcFAjfy2BNnG"}},{"cell_type":"markdown","source":["## 0.1 Preparation"],"metadata":{"id":"ld4bNcT3u3yo"}},{"cell_type":"code","metadata":{"id":"p5a0t1vX_toU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647607489713,"user_tz":-480,"elapsed":6618,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"f4468a76-e9ff-4853-c369-077801258d38"},"source":["import os\n","import csv\n","import torch\n","import numpy as np\n","import pandas as pd\n","\n","from PIL import Image\n","from google.colab import drive\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torch.nn as nn\n","\n","from tqdm.notebook import tqdm\n","\n","import matplotlib.pyplot as plt\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Use device:\",device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Use device: cuda:0\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"UBUxdgl-THIA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647607515730,"user_tz":-480,"elapsed":20894,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"d357aa8b-eedb-4468-ba03-0a904fa6c0b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"3MHZYBJkQWRZ"},"source":["##############################################\n","# Decide your work and save path\n","##############################################\n","DataPath = '/content/hw2_ex/'\n","SavePath = '/content/drive/MyDrive/Colab Notebooks/homework2/'\n","\n","os.makedirs(DataPath, exist_ok = True)\n","os.makedirs(SavePath, exist_ok = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_nJ97y2JXsn"},"source":["## 0.2 Download DataSet: Cat and Dog\n","\n","<p align=\"center\">\n","<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/3362/media/woof_meow.jpg\" width=\"400\">\n","</p>"]},{"cell_type":"code","metadata":{"id":"ekxnEOd-AqGL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647607548424,"user_tz":-480,"elapsed":26788,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"4db7b839-de3d-4d78-bcec-e0b89b9d8e50"},"source":["if not os.path.isdir(DataPath+'/dataset'):\n","    !pip install --upgrade --no-cache-dir gdown\n","    !gdown --id 1hj2zrZI3Nd-C6nlGOE1crgR_gnpoKHQh --output 'dataset.zip'\n","    !unzip -q dataset.zip -d '/content/hw2_ex' # the -d should be the same as DataPath\n","\n","else:\n","    print(\"File already exists.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.2)\n","Collecting gdown\n","  Downloading gdown-4.4.0.tar.gz (14 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.63.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n","Building wheels for collected packages: gdown\n","  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14774 sha256=3651986fc2076eaed4bbda34b4fe3585dcf339bf0b29b318d089e21bf58cee17\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-uvp80n3i/wheels/fb/c3/0e/c4d8ff8bfcb0461afff199471449f642179b74968c15b7a69c\n","Successfully built gdown\n","Installing collected packages: gdown\n","  Attempting uninstall: gdown\n","    Found existing installation: gdown 4.2.2\n","    Uninstalling gdown-4.2.2:\n","      Successfully uninstalled gdown-4.2.2\n","Successfully installed gdown-4.4.0\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1oOpIiMHoMpSqUBUBxZLC7QdhqsB-v3kS\n","To: /content/dataset.zip\n","100% 571M/571M [00:06<00:00, 94.4MB/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"bOwWp6phJxUr"},"source":["# 1 Data Preprocess\n","In this chapter, we will learn how to preprocess image data.\n","\n","Using pandas to preprocess, it is a good module for data analytic.\n","\n","Using dataset and dataloader from pytorch to setup training dataflow."]},{"cell_type":"markdown","metadata":{"id":"3vbSNxQJgu-c"},"source":["## 1.1 Pandas DataFrame\n",">[documentation](https://pandas.pydata.org/docs/index.html)\n",">\n",">Pandas is a Python library used for working with data sets.\n","It has functions for analyzing, cleaning, exploring, and manipulating data.\n",">\n",">The name \"Pandas\" has a reference to both \"Panel Data\", and \"Python Data Analysis\" and was created by Wes McKinney in 2008.\n","\n","\n","<p align=\"center\">\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Pandas_logo.svg/1200px-Pandas_logo.svg.png\" width=\"600\">\n","</p>"]},{"cell_type":"code","metadata":{"id":"2gsa5EGSVASI","colab":{"base_uri":"https://localhost:8080/","height":174},"executionInfo":{"status":"ok","timestamp":1647607569339,"user_tz":-480,"elapsed":5,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"034fcfbd-8ad3-486e-caf9-4924d9726c89"},"source":["import pandas as pd\n","\n","df = pd.read_csv(os.path.join(DataPath, 'train.csv'))\n","df.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          img  label\n","count   20000  20000\n","unique  20000      2\n","top     0.jpg    cat\n","freq        1  10000"],"text/html":["\n","  <div id=\"df-60dd3561-3016-4f69-a7d9-02e24407ee78\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>img</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>20000</td>\n","      <td>20000</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>20000</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>0.jpg</td>\n","      <td>cat</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>1</td>\n","      <td>10000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60dd3561-3016-4f69-a7d9-02e24407ee78')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-60dd3561-3016-4f69-a7d9-02e24407ee78 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-60dd3561-3016-4f69-a7d9-02e24407ee78');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"SO_h4nWsVAH0","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1647579865654,"user_tz":-480,"elapsed":6,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"48e5bfbc-9f4d-4ae9-b70f-8ce21de22827"},"source":["df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     img label\n","0  0.jpg   cat\n","1  1.jpg   cat\n","2  2.jpg   cat\n","3  3.jpg   cat\n","4  4.jpg   cat"],"text/html":["\n","  <div id=\"df-4262acf9-7a96-48cb-af56-795380b63c98\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>img</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.jpg</td>\n","      <td>cat</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.jpg</td>\n","      <td>cat</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.jpg</td>\n","      <td>cat</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.jpg</td>\n","      <td>cat</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.jpg</td>\n","      <td>cat</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4262acf9-7a96-48cb-af56-795380b63c98')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4262acf9-7a96-48cb-af56-795380b63c98 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4262acf9-7a96-48cb-af56-795380b63c98');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"UAVIa4tTVher","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647480637913,"user_tz":-480,"elapsed":12,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"6bf1ad6b-7d0f-4f30-9e42-90987883047a"},"source":["df['label']=='cat'"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0         True\n","1         True\n","2         True\n","3         True\n","4         True\n","         ...  \n","19995    False\n","19996    False\n","19997    False\n","19998    False\n","19999    False\n","Name: label, Length: 20000, dtype: bool"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"wzUDroffU_rU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647480637914,"user_tz":-480,"elapsed":13,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"64d02cfd-6a7f-4a98-90b3-e273671a5635"},"source":["df['img'][df['label']=='dog']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000    10000.jpg\n","10001    10001.jpg\n","10002    10002.jpg\n","10003    10003.jpg\n","10004    10004.jpg\n","           ...    \n","19995    19995.jpg\n","19996    19996.jpg\n","19997    19997.jpg\n","19998    19998.jpg\n","19999    19999.jpg\n","Name: img, Length: 10000, dtype: object"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"F7oMlZNeU_Mp","colab":{"base_uri":"https://localhost:8080/","height":38},"executionInfo":{"status":"ok","timestamp":1647480637915,"user_tz":-480,"elapsed":13,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"bcc5c77c-bc31-48ed-d1a7-82bd9cef0ffa"},"source":["df.at[10,'img']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'10.jpg'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"w7Vh4XJEIOMa","executionInfo":{"status":"ok","timestamp":1647577905075,"user_tz":-480,"elapsed":416,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"d80e4e46-39e2-4352-f08d-ab6f54d186b1"},"source":["df['imgpath'] = DataPath + df['img'].astype(str)\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     img label                imgpath\n","0  0.jpg   cat  /content/hw2_ex/0.jpg\n","1  1.jpg   cat  /content/hw2_ex/1.jpg\n","2  2.jpg   cat  /content/hw2_ex/2.jpg\n","3  3.jpg   cat  /content/hw2_ex/3.jpg\n","4  4.jpg   cat  /content/hw2_ex/4.jpg"],"text/html":["\n","  <div id=\"df-4e2018ec-2cc9-4789-bbe2-b496f8f93541\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>img</th>\n","      <th>label</th>\n","      <th>imgpath</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.jpg</td>\n","      <td>cat</td>\n","      <td>/content/hw2_ex/0.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.jpg</td>\n","      <td>cat</td>\n","      <td>/content/hw2_ex/1.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.jpg</td>\n","      <td>cat</td>\n","      <td>/content/hw2_ex/2.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.jpg</td>\n","      <td>cat</td>\n","      <td>/content/hw2_ex/3.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.jpg</td>\n","      <td>cat</td>\n","      <td>/content/hw2_ex/4.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e2018ec-2cc9-4789-bbe2-b496f8f93541')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4e2018ec-2cc9-4789-bbe2-b496f8f93541 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4e2018ec-2cc9-4789-bbe2-b496f8f93541');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["Class = {'cat':0, 'dog':1}\n","df['category'] = df['label'].astype('category').cat.codes\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"1GCy2FBORvtX","executionInfo":{"status":"ok","timestamp":1647577972453,"user_tz":-480,"elapsed":18,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"7d4c1b17-0110-4397-b80e-618441bfb473"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             img label                    imgpath  category\n","0          0.jpg   cat      /content/hw2_ex/0.jpg         0\n","1          1.jpg   cat      /content/hw2_ex/1.jpg         0\n","2          2.jpg   cat      /content/hw2_ex/2.jpg         0\n","3          3.jpg   cat      /content/hw2_ex/3.jpg         0\n","4          4.jpg   cat      /content/hw2_ex/4.jpg         0\n","...          ...   ...                        ...       ...\n","19995  19995.jpg   dog  /content/hw2_ex/19995.jpg         1\n","19996  19996.jpg   dog  /content/hw2_ex/19996.jpg         1\n","19997  19997.jpg   dog  /content/hw2_ex/19997.jpg         1\n","19998  19998.jpg   dog  /content/hw2_ex/19998.jpg         1\n","19999  19999.jpg   dog  /content/hw2_ex/19999.jpg         1\n","\n","[20000 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-63f03b43-3341-4e50-b639-02dba6524093\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>img</th>\n","      <th>label</th>\n","      <th>imgpath</th>\n","      <th>category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.jpg</td>\n","      <td>cat</td>\n","      <td>/content/hw2_ex/0.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.jpg</td>\n","      <td>cat</td>\n","      <td>/content/hw2_ex/1.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.jpg</td>\n","      <td>cat</td>\n","      <td>/content/hw2_ex/2.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.jpg</td>\n","      <td>cat</td>\n","      <td>/content/hw2_ex/3.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.jpg</td>\n","      <td>cat</td>\n","      <td>/content/hw2_ex/4.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>19995</th>\n","      <td>19995.jpg</td>\n","      <td>dog</td>\n","      <td>/content/hw2_ex/19995.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19996</th>\n","      <td>19996.jpg</td>\n","      <td>dog</td>\n","      <td>/content/hw2_ex/19996.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19997</th>\n","      <td>19997.jpg</td>\n","      <td>dog</td>\n","      <td>/content/hw2_ex/19997.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19998</th>\n","      <td>19998.jpg</td>\n","      <td>dog</td>\n","      <td>/content/hw2_ex/19998.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19999</th>\n","      <td>19999.jpg</td>\n","      <td>dog</td>\n","      <td>/content/hw2_ex/19999.jpg</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20000 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63f03b43-3341-4e50-b639-02dba6524093')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-63f03b43-3341-4e50-b639-02dba6524093 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-63f03b43-3341-4e50-b639-02dba6524093');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"KrC2MQOKWu78","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647577991295,"user_tz":-480,"elapsed":327,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"1159c6f6-e98c-4214-8bdb-14bbd4f31819"},"source":["df.iloc[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["img                         0.jpg\n","label                         cat\n","imgpath     /content/hw2_ex/0.jpg\n","category                        0\n","Name: 0, dtype: object"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"HM_EW1ffg7ql"},"source":["## 1.2 Preprocess data to list\n","\n","We compare two kinds of way for preprocessing.\n","We can find that using the same framework for preprocessing would be efficient."]},{"cell_type":"code","metadata":{"id":"cLxAAXD2J2Cd"},"source":["##############################################\n","# Create a list for downloaded files,\n","#  including the path of the image \n","#  and the corresponding label.\n","##############################################\n","import time\n","\n","\n","train_img_path = os.path.join(DataPath, 'train_dataset/')\n","df = pd.read_csv(os.path.join(DataPath, 'train.csv'))\n","\n","# small experiment\n","start_time = time.time()\n","\n","Class = {'cat':0, 'dog':1}\n","imgpath = []\n","imglabel = []\n","\n","# Load csv and add path\n","for idx, row in df.iterrows():\n","    imgpath.append(os.path.join(train_img_path, str(row[\"img\"])))\n","    imglabel.append(Class[row[\"label\"]])\n","print(f\"Process with for loop in {(time.time()-start_time):.3f} s\\n\\n\")\n","\n","idx = np.random.randint(len(imgpath), size=10)\n","print(np.array(imgpath)[idx])\n","print(np.array(imglabel)[idx])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# small experiment\n","start_time = time.time()\n","imgpath = (train_img_path + df['img'].astype(str)).tolist()\n","imglabel = df['label'].astype('category').cat.codes.tolist()\n","print(f\"Process with pandas framework in {(time.time()-start_time):.3f} s\\n\\n\")\n","\n","print(np.array(imgpath)[idx])\n","print(np.array(imglabel)[idx])"],"metadata":{"id":"1dZO35gxaOKb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"23_htZkLhMV8"},"source":["## 1.3 Pytorch Dataset & Dataloaders\n",">[documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",">\n",">Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives:  `torch.utils.data.DataLoader`  and  `torch.utils.data.Dataset`  that allow you to use pre-loaded datasets as well as your own data. \n",">\n",">Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n","\n","\n","<p align=\"center\">\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/PyTorch_logo_black.svg/2560px-PyTorch_logo_black.svg.png\" width=\"600\">\n","</p>"]},{"cell_type":"markdown","source":["### 1.3.1 torchvision.transforms\n","\n",">[documentation](https://pytorch.org/vision/stable/transforms.html)\n",">\n",">`torchvision.transforms` help us to transform image to tensor and can also help us apply augmentation, for more robust training.\n","\n","<p align=\"center\">\n","<img src=\"https://pytorch.org/vision/stable/_images/sphx_glr_plot_transforms_024.png\" width=\"600\">\n","</p>\n"],"metadata":{"id":"PaknUE_-JF0i"}},{"cell_type":"code","metadata":{"id":"1zHStfuBaKNJ"},"source":["# you need change image to tensor\n","from torchvision import transforms\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    ])\n","valid_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.3.2 torch.utils.data.Dataset"],"metadata":{"id":"zzZ0sJ_1BTwp"}},{"cell_type":"code","metadata":{"id":"_ohR6Ch4TN4I"},"source":["##############################################\n","# Use the list you created above\n","#  to create a class for DataLoader.\n","##############################################\n","from torch.utils.data import Dataset, DataLoader\n","\n","class dataset(Dataset):\n","    def __init__(self, imgpath, csvpath, transform = valid_transform):\n","        # --------------------------------------------\n","        # Initialize paths, transforms, and so on\n","        # --------------------------------------------\n","        df = pd.read_csv(csvpath)\n","        self.images = (imgpath + df['img'].astype(str)).tolist()\n","        self.label = df['label'].astype('category').cat.codes.tolist()\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","        # --------------------------------------------\n","        # 1. Read from file (using numpy.fromfile, PIL.Image.open)\n","        # 2. Preprocess the data (torchvision.Transform).\n","        # 3. Return the data (e.g. image and label)\n","        # --------------------------------------------\n","        imgpath = self.images[index]\n","        img = Image.open(imgpath).convert('RGB')\n","        label = self.label[index]\n","        img = self.transform(img)\n","\n","        return img, label\n","        \n","    def __len__(self):\n","        # --------------------------------------------\n","        # Indicate the total size of the dataset\n","        # --------------------------------------------\n","        return len(self.images)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sY8h9sALfSpi"},"source":["from torch.utils.data import random_split\n","\n","imgpath = os.path.join(DataPath, 'train_dataset/')\n","csvpath = os.path.join(DataPath, 'train.csv')\n","\n","trainset = dataset(imgpath, csvpath)\n","train_set_size = int(len(trainset) * 0.7)\n","valid_set_size = int(len(trainset) * 0.15)\n","test_set_size = len(trainset) - train_set_size - valid_set_size\n","\n","trainset, validset, testset = random_split(trainset, [train_set_size, valid_set_size, test_set_size])\n","\n","trainset.transform = train_transform\n","validset.transform = valid_transform\n","testset.transform = valid_transform\n","\n","print(f'trainset: {len(trainset)}\\nvalidset: {len(validset)}\\ntestset: {len(testset)}')\n","\n","idx = np.random.randint(len(trainset))\n","print(f'{idx:5d}/{len(trainset)} : {trainset[idx]}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.3.3 torch.utils.data.DataLoader"],"metadata":{"id":"Oa8YpgKkBeS8"}},{"cell_type":"code","source":["# Loaded Datasets to DataLoaders\n","\n","# batch_size also affect training step.\n","# higher: faster, stable\n","#   but inprecise on optimize, may use large memory\n","\n","trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers = 2)\n","validloader = DataLoader(validset, batch_size=64, shuffle=False, num_workers = 2)\n","testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers = 2)"],"metadata":{"id":"SKQaTsC4x7Qm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fP8kFu3mWmJo"},"source":["# 2 CNN Model"]},{"cell_type":"markdown","metadata":{"id":"ulMPimGuhXJ6"},"source":["### 2.1 Convolutional layers\n",">[document](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",">\n",">Start to build our first CNN Model!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SyoU-VMU2Gic","executionInfo":{"status":"ok","timestamp":1647607639345,"user_tz":-480,"elapsed":265,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"efc63ee8-e9f0-4a4e-e32e-75d7bb889f79"},"source":["import torch.nn as nn\n","\n","model = nn.Sequential(\n","    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=3),\n","    )\n","\n","result = model(torch.rand((64 ,3, 28, 28)))\n","print(result.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 32, 8, 8])\n"]}]},{"cell_type":"code","metadata":{"id":"hkyaa7nQXvAt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647607644222,"user_tz":-480,"elapsed":2788,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"d93621f6-7dd7-40cb-9654-69b913097906"},"source":["model = nn.Sequential(\n","    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","    nn.Conv2d(in_channels=16, out_channels=64, kernel_size=5),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=3, stride=2),\n","    \n","    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5),\n","    nn.ReLU(),\n","    nn.MaxPool2d(kernel_size=2, stride=2),\n","    )\n","result = model(torch.rand((3, 3, 512, 1024)))\n","print(result.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 128, 60, 124])\n"]}]},{"cell_type":"markdown","source":["<p align=\"center\">\n","<img src=\"https://1.bp.blogspot.com/-db1Bv-YnKTo/XkrIvCYFtEI/AAAAAAAAArA/2b0zbD29CQML4IKxp_1Zng_r0ioaOCZkgCEwYBhgL/s1600/appendix_C_conv_formula.png\" width=\"600\">\n","</p>\n"],"metadata":{"id":"RxL_AFx3LSDh"}},{"cell_type":"code","metadata":{"id":"KbsaLdr7qcbL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647607649770,"user_tz":-480,"elapsed":1633,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"4f626810-007b-47a0-fac4-bdfca87e14f6"},"source":["data_flow = torch.rand((3, 3, 512, 1024))\n","print(data_flow.shape)\n","\n","data_flow = nn.Conv2d(3,16,5)(data_flow)\n","print(data_flow.shape)\n","\n","data_flow = nn.MaxPool2d(2,2)(data_flow)\n","print(data_flow.shape)\n","\n","data_flow = nn.Conv2d(16,64,5)(data_flow)\n","print(data_flow.shape)\n","\n","data_flow = nn.MaxPool2d(2,2)(data_flow)\n","print(data_flow.shape)\n","\n","data_flow = nn.Conv2d(64,128,5)(data_flow)\n","print(data_flow.shape)\n","\n","print((data_flow.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 3, 512, 1024])\n","torch.Size([3, 16, 508, 1020])\n","torch.Size([3, 16, 254, 510])\n","torch.Size([3, 64, 250, 506])\n","torch.Size([3, 64, 125, 253])\n","torch.Size([3, 128, 121, 249])\n","torch.Size([3, 128, 121, 249])\n"]}]},{"cell_type":"markdown","metadata":{"id":"H2Y1Fm0shbDF"},"source":["### 2.2 Convolutional model\n","\n","Try to implement LeNet-5 with pytorch !\n","\n","(but using `nn.ReLU()` as activation function)\n","\n","<p align=\"center\">\n","<img src=\"https://cdn-images-1.medium.com/max/1024/1*DMcPgeekUftwk0GTMcNawg.png\" width=\"900\">\n","</p>"]},{"cell_type":"code","metadata":{"id":"0Y011xk0Wr4r"},"source":["##############################################\n","# Build your model here!\n","# \n","# Practice:\n","#   Try to implement LeNet-5 with pytorch !\n","##############################################\n","class trainmodel(nn.Module):\n","    def __init__(self):\n","        super(trainmodel, self).__init__()\n","        self. conv = nn.Sequential(\n","            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1),\n","            nn.ReLU(),\n","            nn.AvgPool2d(kernel_size=2, stride=2),\n","            # To Do\n","            )\n","        self.cls = nn.Sequential(\n","            nn.Linear(5*5*16,120),\n","            # To Do\n","            nn.Linear(84,2),\n","            nn.Softmax(dim=1),\n","            )\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = x.view(-1,5*5*16)       # Change if you modify network\n","        x = self.cls(x)\n","        return x\n","\n","model = trainmodel()\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sPzHD1mPRv0A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647609960463,"user_tz":-480,"elapsed":261,"user":{"displayName":"N26100668謝旻恩","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16602044497544319963"}},"outputId":"b6691a6a-bc11-4080-ce1a-aed9944235b1"},"source":["## test your model sequential output\n","conv = nn.Sequential(\n","    nn.Conv2d(3,16,3),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2,2),\n","    nn.Conv2d(16,64,3),\n","    nn.ReLU(),\n","    nn.MaxPool2d(3,2),\n","    nn.Conv2d(64,128,3),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2,2),\n","    )\n","batch = torch.rand(3,3,224,224)    \n","result = conv(batch) #It is your img input\n","print(result.shape)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 64, 10, 10])\n"]}]},{"cell_type":"markdown","source":["## 2.3 How to improve Network?\n","\n","### Layer\n","- Adjustment of convolution/pooling layer\n","- [Activation Layer](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n","- Global [Average](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html#torch.nn.AdaptiveAvgPool2d)/[Max](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d) Pooling\n","\n","### Training Robustness\n","- [Augmentation](https://pytorch.org/vision/stable/transforms.html)\n","- [Batch Normalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d)\n","- [Dropout Layers](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)\n","\n","### Optimizer: attempt to find global minimum\n","- [SGD with momentum](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n","- [ADAM](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)\n","- [torch.optim](https://pytorch.org/docs/stable/optim.html)\n","\n","### Learning Rate Sceduler\n","- [Cosine Annealing Learning Rate](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR)\n"],"metadata":{"id":"7sjLe7FiN08I"}},{"cell_type":"code","source":["##############################################\n","# Build your model here!\n","# \n","# Practice:\n","#   Add data augmentation !\n","##############################################\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    # Todo\n","    transforms.ToTensor(),\n","    ])\n","\n","trainset.transform = train_transform\n","trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers = 2)"],"metadata":{"id":"hd3ZFziQQ2SF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##############################################\n","# Build your model here!\n","# \n","# Practice:\n","#   Improve your own Model!\n","##############################################\n","\n","class trainmodel(nn.Module):\n","    def __init__(self):\n","        super(trainmodel, self).__init__()\n","        # To do\n","\n","    def forward(self, x):\n","        # To do\n","        return x\n","\n","model = trainmodel()\n","model.to(device)\n","\n","model = trainmodel()\n","model.to(device)"],"metadata":{"id":"zfZ5rUygNfQv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fhnbd8kKmwmr"},"source":["# 3 Training Routine"]},{"cell_type":"markdown","source":["## 3.1 Training module"],"metadata":{"id":"BDXcqsa-vv6n"}},{"cell_type":"code","source":["def train(model, trainloader, optimizer, criterion):\n","    # keep track of training loss\n","    train_loss = 0.0\n","    train_correct = 0\n","    \n","    # train the model \n","    model.train()\n","    for data, target in tqdm(trainloader):\n","        # move tensors to GPU if CUDA is available\n","        data, target = data.to(device), target.to(device)\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update training loss\n","        train_loss += loss.item()*data.size(0)\n","        # update training Accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        train_correct += (predicted == target).sum().item()\n","\n","    return train_loss/len(trainloader.dataset), train_correct/len(trainloader.dataset)"],"metadata":{"id":"a_Tl-as8c5k-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test(model, testloader, criterion):\n","    # keep track of validation loss\n","    valid_loss = 0.0\n","    valid_correct = 0\n","\n","    # evaluate the model \n","    model.eval()\n","    for data, target in tqdm(testloader):\n","        # move tensors to GPU if CUDA is available\n","        data, target = data.to(device), target.to(device)\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # update average validation loss \n","        valid_loss += loss.item()*data.size(0)\n","        # update validation Accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        valid_correct += (predicted == target).sum().item()\n","\n","    return valid_loss/len(testloader.dataset), valid_correct/len(testloader.dataset)"],"metadata":{"id":"FzlwQDzUvoG-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def modeltrain(model, trainloader, validloader, testloader, optimizer, criterion, epochs, save_model_path, earlystop=4):\n","    history = {\n","        'trainloss' : [],\n","        'trainacc' : [],\n","        'validloss' : [],\n","        'validacc' : [],\n","    }\n","    state = {\n","        'epoch' : 0,\n","        'state_dict' : model.state_dict(),\n","        'trainloss' : 10000,\n","        'trainacc' : 0,\n","        'validloss' : 10000,\n","        'validacc' : 0,\n","    }\n","    valid_loss_min = 10000\n","    trigger = 0\n","    for epoch in range(epochs):\n","        print(f'running epoch: {epoch+1}')\n","        trainloss, trainacc = train(model, trainloader, optimizer, criterion)\n","        validloss, validacc = test(model, validloader, criterion)\n","\n","        # print training/validation statistics \n","        history['trainloss'].append(trainloss)\n","        history['trainacc'].append(trainacc)\n","        history['validloss'].append(validloss)\n","        history['validacc'].append(validacc)\n","        print(f'Training Loss  : {trainloss:.4f}\\t\\tTraining Accuracy  : {trainacc:.4f}')\n","        print(f'Validation Loss: {validloss:.4f}\\t\\tValidation Accuracy: {validacc:.4f}')\n","        \n","        # save model if validation loss has decreased\n","        if validloss <= valid_loss_min:\n","            print(f'Validation loss decreased ({valid_loss_min:.4f} --> {validloss:.4f}).  Saving model ...\\n')\n","            state['epoch'] = epoch\n","            state['state_dict'] = model.state_dict()\n","            state['trainloss'] = trainloss\n","            state['trainacc'] = trainacc\n","            state['validloss'] = validloss\n","            state['validacc'] = validacc\n","\n","            torch.save(state, save_model_path)\n","            valid_loss_min = validloss\n","            trigger = 0\n","        # if model dont improve for 5 times, interupt.\n","        else:\n","            trigger += 1\n","            print(f'Validation loss increased ({valid_loss_min:.4f} --> {validloss:.4f}). Trigger {trigger}/{earlystop}\\n')\n","            if trigger == earlystop:\n","                break\n","    print('\\nTest Evaluate:')\n","    testloss, testacc = test(model, testloader, criterion)\n","    state['testloss'] = testloss\n","    state['testacc'] = testacc\n","    torch.save(state, save_model_path)\n","    bestepoch = state['epoch']\n","    validloss = state['validloss']\n","    validacc = state['validacc']\n","    print(f'Best model on epoch : {bestepoch}/{epoch}')\n","    print(f'validation loss: {validloss:.4f}\\t\\t validation acc : {validacc:.4f}')\n","    print(f'test loss      : {testloss:.4f}\\t\\t test acc \\t: {testacc:.4f}')\n","    return history"],"metadata":{"id":"kYGZ-S5rcOnn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2 Start Training!"],"metadata":{"id":"QpnDkiZ8v2KZ"}},{"cell_type":"code","metadata":{"id":"-sIruMAzZD43"},"source":["from tqdm.notebook import tqdm\n","import torch.nn as nn\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n","n_epochs = 1\n","save_model_path = os.path.join(SavePath, '/model_weight.pth')\n","\n","history = modeltrain(\n","        model = model,\n","        trainloader = trainloader,\n","        validloader = validloader,\n","        testloader = testloader,\n","        optimizer = optimizer,\n","        criterion = criterion,\n","        epochs = n_epochs,\n","        save_model_path = save_model_path\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.3 Plot the result"],"metadata":{"id":"K3c1wH-SwEOB"}},{"cell_type":"code","source":["import json\n","save_history = json.dumps(history)\n","with open(os.path.join(SavePath, 'history.json'), 'w') as f:\n","    json.dump(save_history, f)"],"metadata":{"id":"-S0juEEhHKhw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","def plot(name, savedir, trainhistory, validhistory):\n","    plt.figure(figsize=(10,5))\n","    plt.plot(trainhistory, label = 'train')\n","    plt.plot(validhistory,  label = 'valid')\n","    plt.title(name)\n","    plt.xlabel(\"epochs\")\n","    plt.show()\n","    plt.savefig(savedir)"],"metadata":{"id":"KuEqmcn3wJf-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot('Training Loss', os.path.join(SavePath,'loss.png'), history['trainloss'], history['validloss'])"],"metadata":{"id":"1_e8wpHHxwsv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot('Training Accuracy', os.path.join(SavePath,'acc.png'), history['trainacc'], history['validacc'])"],"metadata":{"id":"eyXjiIvjxugt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLeaMgDOvdtQ"},"source":["# 4 Inference testing data"]},{"cell_type":"markdown","metadata":{"id":"uwSmzwfygx3d"},"source":["### 4.1 load weight"]},{"cell_type":"code","metadata":{"id":"_WcomH_rmrfn"},"source":["## create model as same as your training \n","model = trainmodel()\n","model.to(device)\n","\n","## load weight\n","state = torch.load(save_model_path)\n","# state['state_dict']\n","model.load_state_dict(state['state_dict'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-v7yKgR2hBzx"},"source":["### 4.2 test and save result"]},{"cell_type":"code","metadata":{"id":"YBTAiNkrnbE4"},"source":["transform = transforms.Compose([\n","       transforms.Resize((224, 224)),\n","       transforms.ToTensor(),\n","])\n","\n","class test_set(Dataset):\n","    def __init__(self, img_path, csv_path, transform = transform):\n","        # --------------------------------------------\n","        # Initialize paths, transforms, and so on\n","        # --------------------------------------------\n","        self.df = pd.read_csv(csv_path)\n","        self.img_path = img_path\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","        # --------------------------------------------\n","        # 1. Read from file (using numpy.fromfile, PIL.Image.open)\n","        # 2. Preprocess the data (torchvision.Transform).\n","        # 3. Return the data (e.g. image and label)\n","        # --------------------------------------------\n","        imgpath = os.path.join(self.img_path, self.df.iloc[index, 0])\n","        img = Image.open(imgpath)\n","        img = self.transform(img)\n","\n","        return img, index\n","        \n","    def __len__(self):\n","        # --------------------------------------------\n","        # Indicate the total size of the dataset\n","        # --------------------------------------------\n","        return len(self.df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5KXHFychA-8"},"source":["import torch.nn.functional as F\n","\n","#load test data\n","testimg_path = os.path.join(DataPath, 'test_dataset')\n","testcvs_path = os.path.join(DataPath, 'test.csv')\n","\n","df = pd.read_csv(testcvs_path)\n","\n","Class = {'cat':0, 'dog':1}\n","inv_Class = dict((v, k) for k, v in Class.items())\n","\n","print(len(df))\n","\n","testset = test_set(testimg_path, testcvs_path)\n","testloader = DataLoader(testset, batch_size=1, shuffle=False,pin_memory=True,num_workers = 2)\n","\n","with torch.no_grad():\n","    for data, idx in tqdm(testloader):\n","        data = data.to(device)\n","        pred = F.softmax(model(data),dim=1)\n","        pred = np.argmax(pred.detach().cpu().numpy(),axis=1)\n","\n","        df.at[idx, 'label'] = inv_Class[int(pred)] # convert predicted integer back to class string, only works when batch = 1\n","\n","df.head()\n","df.to_csv(os.path.join(SavePath, 'result.csv'), encoding='utf-8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uFRbGC-3Twh"},"source":["## Only examples have \"test_ans.csv\"\n","df = pd.read_csv(os.path.join(SavePath, 'test_ans.csv'))\n","ans = df[\"label\"].to_numpy()\n","\n","df = pd.read_csv(SavePath+'/result.csv')\n","output = df[\"label\"].to_numpy()\n","\n","print(\"Accuracy:\",((ans == output).sum().item()/len(df)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5 What Next? [Transfer Learning](https://hackmd.io/@allen108108/H1MFrV9WH)\n","\n","\n","<p align=\"center\">\n","<img src=\"https://miro.medium.com/max/1400/1*Ww3AMxZeoiB84GVSRBr4Bw.png\" width=\"700\">\n","</p>"],"metadata":{"id":"f1VSC_WRRq15"}},{"cell_type":"markdown","source":["### 5.1 Transfer Learning on Pytorch \n","[Transfer learning for computer vision tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n","\n","[Models and pre-trained weights](https://pytorch.org/vision/stable/models.html)\n"],"metadata":{"id":"1f_qvGf-TPTb"}},{"cell_type":"code","source":["\"\"\"\n","import torchvision.models as models\n","\n","resnet18 = models.resnet18(pretrained=True)\n","alexnet = models.alexnet(pretrained=True)\n","squeezenet = models.squeezenet1_0(pretrained=True)\n","vgg16 = models.vgg16(pretrained=True)\n","densenet = models.densenet161(pretrained=True)\n","inception = models.inception_v3(pretrained=True)\n","googlenet = models.googlenet(pretrained=True)\n","shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n","mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n","mobilenet_v3_large = models.mobilenet_v3_large(pretrained=True)\n","mobilenet_v3_small = models.mobilenet_v3_small(pretrained=True)\n","resnext50_32x4d = models.resnext50_32x4d(pretrained=True)\n","wide_resnet50_2 = models.wide_resnet50_2(pretrained=True)\n","mnasnet = models.mnasnet1_0(pretrained=True)\n","efficientnet_b0 = models.efficientnet_b0(pretrained=True)\n","efficientnet_b1 = models.efficientnet_b1(pretrained=True)\n","efficientnet_b2 = models.efficientnet_b2(pretrained=True)\n","efficientnet_b3 = models.efficientnet_b3(pretrained=True)\n","efficientnet_b4 = models.efficientnet_b4(pretrained=True)\n","efficientnet_b5 = models.efficientnet_b5(pretrained=True)\n","efficientnet_b6 = models.efficientnet_b6(pretrained=True)\n","efficientnet_b7 = models.efficientnet_b7(pretrained=True)\n","regnet_y_400mf = models.regnet_y_400mf(pretrained=True)\n","regnet_y_800mf = models.regnet_y_800mf(pretrained=True)\n","regnet_y_1_6gf = models.regnet_y_1_6gf(pretrained=True)\n","regnet_y_3_2gf = models.regnet_y_3_2gf(pretrained=True)\n","regnet_y_8gf = models.regnet_y_8gf(pretrained=True)\n","regnet_y_16gf = models.regnet_y_16gf(pretrained=True)\n","regnet_y_32gf = models.regnet_y_32gf(pretrained=True)\n","regnet_x_400mf = models.regnet_x_400mf(pretrained=True)\n","regnet_x_800mf = models.regnet_x_800mf(pretrained=True)\n","regnet_x_1_6gf = models.regnet_x_1_6gf(pretrained=True)\n","regnet_x_3_2gf = models.regnet_x_3_2gf(pretrained=True)\n","regnet_x_8gf = models.regnet_x_8gf(pretrained=True)\n","regnet_x_16gf = models.regnet_x_16gf(pretrained=True)\n","regnet_x_32gf = models.regnet_x_32gf(pretrained=True)\n","vit_b_16 = models.vit_b_16(pretrained=True)\n","vit_b_32 = models.vit_b_32(pretrained=True)\n","vit_l_16 = models.vit_l_16(pretrained=True)\n","vit_l_32 = models.vit_l_32(pretrained=True)\n","convnext_tiny = models.convnext_tiny(pretrained=True)\n","convnext_small = models.convnext_small(pretrained=True)\n","convnext_base = models.convnext_base(pretrained=True)\n","convnext_large = models.convnext_large(pretrained=True)\n","\n","\"\"\""],"metadata":{"id":"Za4iUS-7SjXn"},"execution_count":null,"outputs":[]}]}