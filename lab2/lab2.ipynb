{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "REPO_ROOT = \"/content/ml-practices\"\n",
    "LAB2_ROOT = os.path.join(REPO_ROOT, \"lab2\")\n",
    "DATASET_ROOT = \"/content/dataset\"\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/colab/hw2/\"\n",
    "TRAIN_DATASET = os.path.join(DATASET_ROOT, \"train_dataset\")\n",
    "TEST_DATASET  = os.path.join(DATASET_ROOT, \"test_dataset\")\n",
    "TRAIN_CSV     = os.path.join(DATASET_ROOT, \"train.csv\")\n",
    "TEST_CSV      = os.path.join(DATASET_ROOT, \"test.csv\")\n",
    "os.makedirs(DRIVE_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(REPO_ROOT):\n",
    "    !git clone https://github.com/bhbbbbb/ml-practices.git\n",
    "    !cd {REPO_ROOT} && git checkout colab\n",
    "    !pip install git+https://github.com/benjs/nfnets_pytorch\n",
    "else:\n",
    "    !cd {REPO_ROOT} && git pull "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(TRAIN_DATASET):\n",
    "    !pip install --upgrade --no-cache-dir gdown\n",
    "    !gdown --id 1hj2zrZI3Nd-C6nlGOE1crgR_gnpoKHQh --output 'dataset.zip'\n",
    "    !unzip -q dataset.zip -d '/content/dataset' # the -d should be the same as DataPath\n",
    "\n",
    "else:\n",
    "    print(\"File already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(LAB2_ROOT)\n",
    "from imgclf.model_utils import ModelUtils\n",
    "from imgclf.dataset import Dataset\n",
    "from imgclf.models import FatLeNet5, FakeVGG16\n",
    "from hw2.utils import DatasetUtils\n",
    "from hw2.config import Hw2Config\n",
    "import torch.cuda\n",
    "# assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, epochs):\n",
    "    df, cat = DatasetUtils.load_csv(TRAIN_CSV, TRAIN_DATASET)\n",
    "    datasets = Dataset.split(df, split_ratio=[0.7, 0.15], config=config)\n",
    "    utils = ModelUtils.start_new_training(model=model, config=config)\n",
    "    utils.train(epochs, *datasets)\n",
    "    utils.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_from(config, model, epochs, checkpoint_path):\n",
    "    df, cat = DatasetUtils.load_csv(TRAIN_CSV, TRAIN_DATASET)\n",
    "    datasets = Dataset.split(df, split_ratio=[0.7, 0.15], config=config)\n",
    "    utils = ModelUtils.load_checkpoint(model=model, config=config, checkpoint_path=checkpoint_path)\n",
    "    utils.train(epochs, *datasets)\n",
    "    utils.plot_history()\n",
    "    return\n",
    "\n",
    "def retrain(config, model, epochs):\n",
    "    df, cat = DatasetUtils.load_csv(TRAIN_CSV, TRAIN_DATASET)\n",
    "    datasets = Dataset.split(df, split_ratio=[0.7, 0.15], config=config)\n",
    "    utils = ModelUtils.load_last_checkpoint(model=model, config=config)\n",
    "    utils.train(datasets, epochs=epochs)\n",
    "    utils.plot_history()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(config, categories: list, model):\n",
    "    df = DatasetUtils.load_test_csv(\n",
    "            csv_path = TEST_CSV,\n",
    "            images_root = TEST_DATASET,\n",
    "        )\n",
    "    dataset = Dataset(df, config=config, mode=\"inference\")\n",
    "    utils = ModelUtils.load_last_checkpoint(model=model, config=config)\n",
    "    df = utils.inference(dataset, categories, confidence=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Hw2Config(log_dir = DRIVE_ROOT)\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(config, FakeVGG16(config), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model (NFnet-F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"/content/F1_haiku.npz\")\n",
    "    !wget https://storage.googleapis.com/dm-nfnets/$F1_haiku.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nfnet.nfnet_model_utils import NfnetModelUtils\n",
    "from nfnet.config import NfnetConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = NfnetConfig(log_dir = DRIVE_ROOT)\n",
    "config.learning_rate = 0.1\n",
    "config.batch_size[\"train\"] = 32\n",
    "config.batch_size[\"eval\"] = 32\n",
    "config.num_workers = 2\n",
    "config.num_class = 10\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nfnet = pretrained_nfnet(\"/content/F1_haiku.npz\")\n",
    "epochs = 50\n",
    "df, cat = DatasetUtils.load_csv(TRAIN_CSV, TRAIN_DATASET)\n",
    "datasets = Dataset.split(df, split_ratio=[0.7, 0.15], config=config)\n",
    "model = NfnetModelUtils.init_model(config)\n",
    "utils = NfnetModelUtils.load_last_checkpoint(model, config)\n",
    "utils.train(epochs, *datasets)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc7f54debfa0ced2642815502d19580d9cc5faeb3690fcabf793ce32f40109c8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
